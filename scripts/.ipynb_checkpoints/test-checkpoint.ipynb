{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba147d37-0c10-423c-8ea0-a28322184327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This paper is about a new model for studying links, which are types of knots. The model is called the random meander link model and it uses meander diagrams. Meander diagrams are pairs of curves in the plane, where one curve is straight and the other curves back and forth over it. The ends of these curves are connected to form a meander diagram. This model allows us to study properties of links using tools from combinatorics, which is a branch of mathematics that deals with counting and organizing objects.\\n\\nThe paper proves several results about the random meander link model. First, it shows that the probability of getting an unlink, which is when the curves in a meander diagram do not intersect, is very low. Second, it proves that it is impossible to obtain every possible link using this model. There will always be some links that cannot be made. However, the model does produce infinitely many distinct links. Finally, the paper calculates the expected number of twists in a random link diagram. This number is related to the geometry of the link in 3-dimensional space.\\n\\nOne advantage of the random meander link model is that it can be translated into a combinatorial problem about pairs of parentheses. This makes it easier to count the number of random meander diagrams using a formula called Catalan numbers. The paper also uses tools from combinatorics to prove certain properties of the links, such as the rarity of unlinks.\\n\\nOverall, the paper introduces a new model for studying links and proves some interesting results using combinatorics. It shows that this model can be a useful tool for understanding the properties of links and their geometric characteristics.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import slack\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from flask import Flask\n",
    "from slackeventsapi import SlackEventAdapter\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import requests\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "env_path = Path('.')/'.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "openai.api_key = token=os.environ['CHAT_TOKEN']\n",
    "\n",
    "def get_html(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    html_content = response.read().decode(\"utf-8\")\n",
    "    return html_content\n",
    "def get_pdf(url):\n",
    "    arxiv_id = url[22:]\n",
    "    url = \"https://arxiv.org/pdf/\" + arxiv_id + \".pdf\"\n",
    "    response = requests.get(url)\n",
    "    # Ensure the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Converting the PDF to a more readable format\n",
    "        with BytesIO(response.content) as open_pdf_file:\n",
    "            pdf = PyPDF2.PdfReader(open_pdf_file)\n",
    "            return pdf\n",
    "    else:\n",
    "        return(\"Pattern not found [Failed match flag]\")\n",
    "def get_pdf_text(url):\n",
    "    arxiv_id = url[22:]\n",
    "    url = \"https://arxiv.org/pdf/\" + arxiv_id + \".pdf\"\n",
    "    response = requests.get(url)\n",
    "    # Ensure the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Converting the PDF to a more readable format\n",
    "        with BytesIO(response.content) as open_pdf_file:\n",
    "            read_pdf = PyPDF2.PdfReader(open_pdf_file)\n",
    "            num_pages = len(read_pdf.pages)\n",
    "            # Step 3: Extracting the text\n",
    "            text_content = \"\"\n",
    "            for i in range(num_pages):\n",
    "                page = read_pdf.pages[i]\n",
    "                text_content += page.extract_text()\n",
    "        return(text_content)\n",
    "    else:\n",
    "        return(\"Pattern not found [Failed match flag]\")\n",
    "def fetch_abstract_from_html(url):\n",
    "    pattern = r'<meta property=\"og:description\" content=\"(.*?)\"/>'\n",
    "    html_content = str(get_html(url))\n",
    "    match = re.search(pattern, html_content, re.DOTALL)\n",
    "    if match:\n",
    "        extracted_content = match.group(1)\n",
    "        return(extracted_content)\n",
    "    else:\n",
    "        return(\"Pdf not found [Failed match flag]\")\n",
    "def fetch_abstract(text):\n",
    "    #print(text)\n",
    "    pattern = r'Abstract(.*?).\\n1'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        extracted_content = match.group(1)\n",
    "        return(extracted_content)\n",
    "    else:\n",
    "        return(\"Conclusion not found [Failed match flag]\")\n",
    "def fetch_conclusion(text):\n",
    "    #print(text)\n",
    "    pattern = r'Conclusion(.*?)Acknowledgments'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        extracted_content = match.group(1)\n",
    "        return(extracted_content)\n",
    "    else:\n",
    "        return(\"Conclusion not found [Failed match flag]\")\n",
    "def fetch_introduction(text):\n",
    "    #eturn(text)\n",
    "    pattern = r'Introduction(.*?).\\n2'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        extracted_content = match.group(1)\n",
    "        return(extracted_content)\n",
    "    else:\n",
    "        return(\"Introduction not found [Failed match flag]\")\n",
    "def remove_newlinechar(user_message):\n",
    "    pattern = r'\\n'\n",
    "    match = re.search(pattern, user_message, re.DOTALL)\n",
    "    if match:\n",
    "        pings = match.group(0)\n",
    "        user_message = user_message.replace(str(pings), \" \")\n",
    "    return(user_message)\n",
    "def fetch_authors_from_html(url):\n",
    "    pattern = r'<meta name=\"citation_author\" content=\"(.*?)\" />'\n",
    "    html_content = str(get_html(url))\n",
    "    match = re.findall(pattern, html_content, re.DOTALL)\n",
    "    if match:\n",
    "        return(match)\n",
    "    else:\n",
    "        return(\"Authors not found [Failed match flag]\")\n",
    "def fetch_title_from_html(url):\n",
    "    pattern = r'<meta name=\"twitter:title\" content=\"(.*?)\"/>'\n",
    "    html_content = str(get_html(url))\n",
    "    match = re.search(pattern, html_content, re.DOTALL)\n",
    "    if match:\n",
    "        extracted_content = match.group(1)\n",
    "        return(extracted_content)\n",
    "    else:\n",
    "        return(\"Title not found [Failed match flag]\")\n",
    "    \n",
    "\n",
    "################################################\n",
    "###########  FUNCTIONS FOR MATEO  ##############\n",
    "################################################\n",
    "def get_title(url):\n",
    "    return fetch_title_from_html(url)\n",
    "def get_authors(url):\n",
    "    return fetch_authors_from_html(url)\n",
    "def get_abstract(url):\n",
    "    return fetch_abstract_from_html(url)\n",
    "def get_introduction(url):\n",
    "    return remove_newlinechar(fetch_introduction(get_pdf_text(url)))\n",
    "def get_conclusion(url):\n",
    "    return fetch_conclusion(remove_newlinechar(get_pdf_text(url)))\n",
    "def fetch_everything(url):\n",
    "    pdf = get_pdf_text(url)\n",
    "    content = {}\n",
    "    content[\"title\"] = get_title(url)\n",
    "    content[\"authors\"] = get_authors(url)\n",
    "    content[\"abstract\"] =  get_abstract(url)\n",
    "    content[\"introduction\"] = fetch_introduction(pdf)\n",
    "    content[\"conclusion\"] =  fetch_conclusion(pdf)\n",
    "    return content\n",
    "test_url = \"https://arxiv.org/abs/2205.03451\"\n",
    "\n",
    "def append_title(meta_prompt, title):\n",
    "    if (\"[Failed match flag]\" not in title):\n",
    "        meta_prompt += \"Summarize the following paper titled \\\"\" + title + \"\\\" \"\n",
    "    else:\n",
    "        meta_prompt += \"Summarize the following paper \"\n",
    "    return meta_prompt\n",
    "def append_authors(meta_prompt, authors):\n",
    "    if (\"[Failed match flag]\" in authors):\n",
    "        return meta_prompt\n",
    "    if (len(authors) == 1):\n",
    "        return meta_prompt + \"writen by \" + authors[0] + \" \"\n",
    "    meta_prompt += \"written by \"\n",
    "    for i in range(len(authors)-1):\n",
    "        meta_prompt += authors[i] + \", \"\n",
    "    meta_prompt += authors[-1] + \" \"\n",
    "    return meta_prompt\n",
    "# Levels = [\"child\", \"teenager\", \"undergraduate\", \"graduate\", \"phd\"]\n",
    "def append_level(meta_prompt, level):\n",
    "    if level == \"child\":\n",
    "        return meta_prompt + \"such that a child could understand it:\\n\"\n",
    "    if level == \"teenager\":\n",
    "        return meta_prompt + \"such that a teenager could understand it:\\n\"\n",
    "    if level == \"undergraduate\":\n",
    "        return meta_prompt + \"at the level of an undergraduate:\\n\"\n",
    "    if level == \"graduate\":\n",
    "        return meta_prompt + \"to a graduate or masters program audience:\\n\"\n",
    "    if level == \"phd\":\n",
    "        return meta_prompt + \"to a phd, do not leave out technicalities:\\n\"\n",
    "def append_abstract(meta_prompt, abstract):\n",
    "    if (\"[Failed match flag]\" in abstract):\n",
    "        return meta_prompt\n",
    "    abstract = remove_newlinechar(abstract)\n",
    "    meta_prompt += \"Abstract: \" + abstract + \"\\n\"\n",
    "    return meta_prompt\n",
    "def append_introduction(meta_prompt, introduction):\n",
    "    if (\"[Failed match flag]\" in introduction):\n",
    "        return meta_prompt\n",
    "    introduction = remove_newlinechar(introduction)\n",
    "    meta_prompt += \"Introduction: \" + introduction + \"\\n\"\n",
    "    return meta_prompt\n",
    "def append_conclusion(meta_prompt, conclusion):\n",
    "    if (\"[Failed match flag]\" in conclusion):\n",
    "        return meta_prompt\n",
    "    conclusion = remove_newlinechar(conclusion)\n",
    "    meta_prompt += \"Conclusion: \" + conclusion + \"\\n\"\n",
    "    return meta_prompt\n",
    "def format_meta_prompt(content, level):\n",
    "    meta_prompt = \"\"\n",
    "    meta_prompt = append_title(meta_prompt, content[\"title\"])\n",
    "    meta_prompt = append_authors(meta_prompt, content[\"authors\"])\n",
    "    meta_prompt = append_level(meta_prompt, level)\n",
    "    meta_prompt += \"\\n\"\n",
    "    meta_prompt = append_abstract(meta_prompt, content[\"abstract\"])\n",
    "    meta_prompt += \"\\n\"\n",
    "    meta_prompt = append_introduction(meta_prompt, content[\"introduction\"])\n",
    "    meta_prompt += \"\\n\"\n",
    "    meta_prompt = append_conclusion(meta_prompt, content[\"conclusion\"])\n",
    "    return meta_prompt\n",
    "\n",
    "\n",
    "\n",
    "##### Function for Mateoooooo #################\n",
    "# for level, select from \"child\", \"teenager\", \"undegraduate\", \"graduate\", \"phd\" \n",
    "def poindexter(url = \"\", filepath = \"\", level = \"child\"):\n",
    "    if url != \"\":\n",
    "        content = fetch_everything(url)\n",
    "    elif filepath != \"\":\n",
    "        content = fetch_everything(url)\n",
    "        print(\"pdf detected\")\n",
    "    \n",
    "    meta_prompt = format_meta_prompt(content, level)\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model = \"gpt-3.5-turbo-16k\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": meta_prompt},\n",
    "    ],\n",
    "    max_tokens = 500,\n",
    "    temperature = 0.8,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28f0d734-9621-47af-ac82-8bcb9d8f0fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '',\n",
       " 'authors': '',\n",
       " 'abstract': '',\n",
       " 'introduction': '\\nSelf-supervised representation learning (SSRL) has wit-\\nnessed a booming era of foundational models (Bommasani\\nyEqual contributions: Zekun Qi <qizekun@gmail.com >and\\nRunpei Dong <runpei.dong@gmail.com >\\x7fInternship at MEGVII\\n1Xi’an Jiaotong University2MEGVII Technology3Tsinghua Uni-\\nversity4Shanghai AI Laboratory5Shanghai Qi Zhi Institute. Corre-\\nspondence to: Kaisheng Ma <kaisheng@mail.tsinghua.edu.cn >,\\nLi Yi<ericyi@mail.tsinghua.edu.cn >.\\nProceedings of the 40thInternational Conference on Machine\\nLearning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyrigh',\n",
       " 'conclusion': 'Conclusion not found [Failed match flag]'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_everything_from_pdf(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        pdf = PyPDF2.PdfReader(f)\n",
    "        num_pages = len(pdf.pages)\n",
    "        text = \"\"\n",
    "        for i in range(num_pages):\n",
    "            page = pdf.pages[i]\n",
    "            text += page.extract_text()\n",
    "    content = {}\n",
    "    content[\"title\"] = \"\"\n",
    "    content[\"authors\"] = \"\"\n",
    "    content[\"abstract\"] =  \"\"\n",
    "    content[\"firstpage\"] = \"\"\n",
    "    content[\"introduction\"] = fetch_introduction(text)\n",
    "    content[\"conclusion\"] =  fetch_conclusion(text)\n",
    "    return content\n",
    "\n",
    "\n",
    "## Test stuff ##\n",
    "test_url = \"https://arxiv.org/abs/2205.03451\"\n",
    "filepath = \"/home/myshell/work/Poindexter/scripts/pdf/\"\n",
    "pdfname = \"2302.02318.pdf\"\n",
    "filepath += pdfname\n",
    "test = \"https://arxiv.org/abs/2205.03451\"\n",
    "fetch_everything_from_pdf(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c24c1c-7add-4eb2-a60b-d94031cdd1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
